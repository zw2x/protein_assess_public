import os
import copy
import json
import argparse
import dataclasses
from collections import defaultdict
from itertools import chain
from typing import List, Optional, Dict
import logging

from tqdm import tqdm
from pathlib import Path

import numpy as np

from src import dockq, tmalign, contact_assess
from src.openstructure import OpenStructureRunner

DOCKQ_ROOT = os.environ.get("DOCKQ_ROOT", None)
assert DOCKQ_ROOT, "DOCKQ_ROOT is not set up"

USALIGN_ROOT = os.environ.get("USALIGN_ROOT", None)
assert USALIGN_ROOT, "DOCKQ_ROOT is not set up"

logger = logging.getLogger(__file__)

def build_dockq_runner(dockq_root: Path = Path(DOCKQ_ROOT)):
    dockq_binary_path = dockq_root.joinpath("DockQ.py")
    assert dockq_binary_path.exists()
    return dockq.DockQRunner(dockq_binary_path)


def build_tmalign_runner(usalign_root: Path = Path(USALIGN_ROOT)):
    usalign_binary_path = usalign_root.joinpath("USalign")
    assert usalign_binary_path.exists()
    return tmalign.TMalignRunner(usalign_binary_path)


def assess_dockq(
    runner,
    model_path: Path,
    native_path: Path,
    score_path: Optional[Path] = None,
    ligand_ids: str = "A",
    receptor_ids: str = "B",
    use_needle: bool = True,
):

    result, _ = runner.run(model_path, native_path, ligand_ids, receptor_ids, use_needle=use_needle)
    if result is None:
        raise RuntimeError("Failed to run DockQ for ", model_path, native_path)

    if score_path is not None:
        with open(score_path, "wt") as fh:
            scores = {"dockq": dataclasses.asdict(result)}
            json.dump(scores, fh)

    return result


KEYS = ["fnat", "fnonnat", "irms", "lrms", "dockq", "mismatch"]


def export_results(results, output_path):
    lines = [",".join(["pdb_id"] + KEYS)]
    for pdb_id, result in results.items():
        line = [pdb_id]
        for key in KEYS:
            line.append(f"{result[key]}")
        line = ",".join(line)
        lines.append(line)

    csv_str = "\n".join(lines)
    with open(output_path, "wt") as fh:
        fh.write(csv_str)

""" Rank models
"""
def read_scores(score_paths, print_scores: bool = True, metric: str = "dockq"):
    all_scores = {}
    sorted_scores = []
    for score_path in score_paths:
        if score_path.exists():
            with open(score_path, "rt") as fh:
                score = json.load(fh)
                if metric not in score:
                    continue
                # assert metric in score, f"{score_path}\n{score}"
                all_scores[score_path.stem] = score
                sorted_scores.append(score[metric])

    if len(all_scores) == 0:
        return

    if print_scores:
        sorted_scores = sorted(sorted_scores)
        print(
            "max",
            sorted_scores[-1],
            "min",
            sorted_scores[0],
            "median",
            np.median(sorted_scores),
            "mean",
            np.mean(sorted_scores),
        )

    return all_scores


def gather_ranked_scores(model_paths, score_paths) -> Optional[List[Dict]]:
    """Gather scores from score paths and rankings fro model paths
    """
    all_scores = read_scores(score_paths, print_scores=False)
    if all_scores is None:
        return
    # Read rankings from original model directories
    output_scores = []
    for model_path in model_paths:
        name = model_path.stem
        rank_path = model_path.parent.joinpath(f"{name}_ranking.json")
        with open(rank_path, "rt") as fh:
            ranking = json.load(fh)
            output_scores.append(
                {
                    "name": name,
                    "score": all_scores[name],
                    "ranking": ranking,
                }
            )

    return output_scores

def rank_models(args):
    if args.model_dir is None:
        model_paths = [args.model_path]
        score_paths = [args.score_path]
    else:
        assert args.score_path is not None
        if not args.score_path.exists():
            return
        assert args.score_path.is_dir(), args.score_path
        model_paths = sorted(args.model_dir.glob("*.pdb"))
        if len(model_paths) == 0:
            return
        score_paths = [args.score_path.joinpath(f"{p.stem}.json") for p in model_paths]

    scores = gather_ranked_scores(model_paths, score_paths)

    return scores

def parse_iface_result(score_path):
    """Parse result (.json) generated by iface-check
    """
    with open(score_path) as fh:
        outputs = json.load(fh)
    return outputs

def parse_ost_result(score_path):
    """Parse result (.json) generated by OpenStructure
    """
    with open(score_path) as fh:
        outputs = json.load(fh)
    _names = list(outputs["result"].keys())
    assert len(_names) == 1
    _name = _names[0]
    _ref_names = list(outputs["result"][_name].keys())
    assert len(_ref_names) == 1
    _ref_name = _ref_names[0]
    outputs = outputs["result"][_name][_ref_name]
    score = {
        "oligo_lddt": outputs["lddt"]["oligo_lddt"]["global_score"],
        "single_chain_lddt": [
            (v["model_chain"], v["global_score"])
            for v in outputs["lddt"]["single_chain_lddt"]
        ],
        "qs_best": outputs["qs_score"]["best_score"],
        "qs_global": outputs["qs_score"]["global_score"],
    }
    return score


""" Assess models
"""
def launch_dockq_runner(
    *,
    native_path,
    model_paths,
    output_model_paths,
    score_paths,
    fix_number: bool = True,
    use_needle: bool = True,
):
    runner = build_dockq_runner()

    for model_path, output_model_path, score_path in tqdm(
        list(zip(model_paths, output_model_paths, score_paths))
    ):
        if score_path and score_path.exists():
            continue
        try:
            if fix_number:
                chain_ids = runner.fix_numbering(model_path, native_path, output_model_path)
            else:
                chain_ids = runner.read_chain_ids(model_path)

            ligand_ids = chain_ids[0]
            receptor_ids = " ".join(chain_ids[1:])

            result = assess_dockq(
                runner,
                output_model_path,
                native_path,
                score_path=score_path,
                ligand_ids=ligand_ids,
                receptor_ids=receptor_ids,
                use_needle=use_needle,
            )

        except RuntimeError as e:
            # print(e)
            print(e)
            continue


def launch_tmalign_runner(
    *,
    native_path,
    model_paths,
    output_model_paths,
    score_paths,
):
    runner = build_tmalign_runner()

    for model_path, output_model_path, score_path in tqdm(
        list(zip(model_paths, output_model_paths, score_paths))
    ):
        if output_model_path and output_model_path.exists():
            input_path = output_model_path
        else:
            input_path = model_path

        result, _ = runner.run_usalign(input_path, native_path)

        if score_path is not None:
            cur_scores = {}
            if score_path.exists():
                with open(score_path, "rt") as fh:
                    cur_scores = json.load(fh)

            if result is not None:
                _scores = dataclasses.asdict(result)
                _scores.pop("msg")
                cur_scores["usalign"] = _scores
            with open(score_path, "wt") as fh:
                json.dump(cur_scores, fh)


def assess_model_dir(args):
    if args.model_dir is None:
        model_paths = [args.model_path]
        output_model_paths = [args.output_model_path]
        score_paths = [args.score_path]
    else:
        model_paths = sorted(args.model_dir.glob("*.pdb"))
        if len(model_paths) == 0:
            return

        output_model_dir = args.output_model_dir
        assert output_model_dir is not None
        output_model_dir.mkdir(exist_ok=True, parents=True)
        output_model_paths = [output_model_dir.joinpath(p.name) for p in model_paths]

        score_paths = [output_model_dir.joinpath(f"{p.stem}.json") for p in model_paths]

    native_path = args.native_path

    if args.run_iface:
        with open(native_path) as fh:
            pdb_str = fh.read()
            ref_dict = contact_assess.parse_atoms_from_pdb(pdb_str)
        ref_contacts, ref_sites = contact_assess.find_residue_contacts(ref_dict)
        for model_path, score_path in zip(model_paths, score_paths):
            if score_path.exists():
                continue
            with open(model_path) as fh:
                pdb_str = fh.read()
                model_dict = contact_assess.parse_atoms_from_pdb(pdb_str)
            model_contacts, model_sites = contact_assess.find_residue_contacts(model_dict)
            contact_mets = contact_assess.compute_contact_metrics(
                ref_contacts, model_contacts
            )
            site_mets = contact_assess.compute_site_metrics(ref_sites, model_sites)
            iface_mets = {"iface_contact": contact_mets, "iface_site": site_mets}
            with open(score_path, "wt") as fh:
                json.dump(iface_mets, fh)

    elif args.run_openstructure:
        runner = OpenStructureRunner(args.openstructure_image)
        runner.run_batch(model_paths, native_path, score_paths, output_model_dir)

    else:
        launch_dockq_runner(
            native_path=native_path,
            model_paths=model_paths,
            output_model_paths=output_model_paths,
            score_paths=score_paths,
        )

        launch_tmalign_runner(
            native_path=native_path,
            model_paths=model_paths,
            output_model_paths=output_model_paths,
            score_paths=score_paths,
        )


def get_args():
    """
    Assess a model pdb `model_path` or all model pdbs in `model_dir` against a native pdb `native_path`.
    The aligned model(s) will be saved to `output_model_path` or in `output_model_dir`.
    All the score will be saved to `score_path` if provided.

    Examples:
        To fix numbering and run dockq and usalign
        ```
        python --model-root /path/to/preds --output-model-root /path/to/results \
            --native-root /path/to/gt 
        ```

        To run iface
        ```
        python run_structure_assess.py --model-root /path/to/preds --native-root /path/to/gt \
            --output-model-root /path/to/iface_result --run-iface
        ```

        To generate rank-result file for predicted models
        ```
        python run_structure_assess.py --model-root /path/to/preds --score-path /path/to/results \
            --rank-result-path /path/to/result.json  --rank-model
        ```
    """
    parser = argparse.ArgumentParser()

    parser.add_argument("--model-path", type=Path)
    parser.add_argument("--model-dir", type=Path)
    parser.add_argument("--native-path", type=Path)
    parser.add_argument("--output-model-path", type=Path)
    parser.add_argument("--output-model-dir", type=Path)

    parser.add_argument("--score-path", type=Path)

    parser.add_argument("--rank-model", action="store_true", help="Only rank models.")
    parser.add_argument("--rank-result-path", type=Path)

    parser.add_argument(
        "--fix-model",
        action="store_true",
        help="Only fix model numbering againt native.",
    )

    # Run openstructure
    parser.add_argument(
        "--run-openstructure",
        action="store_true",
        help="Run openstructure docker",
    )
    parser.add_argument(
        "--openstructure-image",
        type=str,
        help="Docker image id to openstructure",
    )
    parser.add_argument(
        "--rank-openstructure",
        action="store_true",
        help="Rank openstructure results",
    )
    parser.add_argument("--openstructure-result-path", type=Path)

    # Compute interface contact related scores
    parser.add_argument("--run-iface", action="store_true", help="Run interface check")
    parser.add_argument("--rank-iface", action="store_true", help="Rank assessed iface results")
    parser.add_argument("--iface-result-path", type=Path)

    # Batch mode
    parser.add_argument("--model-root", type=Path)
    parser.add_argument("--result-name", type=str, default="results")

    parser.add_argument("--output-model-root", type=Path)

    parser.add_argument("--native-root", type=Path)

    # Compare results generated by different protocol
    parser.add_argument(
        "--compare-results", action="store_true", help="Only compare results."
    )

    parser.add_argument("--all-result-paths", type=str)
    parser.add_argument("--tgt-csv", type=Path)

    return parser.parse_args()


def sort_values_by_confidence(values, metric: str = "confidence"):
    return sorted(values, key=lambda x: x["ranking"][metric], reverse=True)


def sort_values_by_score(values, assess_tool: str = "dockq", metric: str = "dockq"):
    return sorted(values, key=lambda x: x["score"][assess_tool][metric], reverse=True)


def mix_results(result_paths, topk_list: List[int] = [1, 5, -1], tgt_file: Optional[Path] = None):
    allowed_tgts = None
    if tgt_file is not None:
        allowed_tgts = set()
        with open(tgt_file) as fh:
            for line in fh:
                line = line.strip()
                if line and not line.startswith("#"):
                    fields = line.split(",")
                    tgt = fields[0]
                    allowed_tgts.add(tgt)

    results = defaultdict(dict)
    # Group results from different models under the same target
    for p in result_paths:
        grp_name = p.stem
        with open(p, "rt") as fh:
            result = json.load(fh)
            for tgt, v in result.items():
                if allowed_tgts is not None and tgt not in allowed_tgts:
                    continue
                results[tgt][grp_name] = v

    final_results = defaultdict(dict)
    tgt_names = sorted(results.keys())
    for tgt in tgt_names:
        named_result = results[tgt]
        mixed_vs = []
        for grp_name, vs in named_result.items():
            # Rank by confidence
            renamed_vs = copy.deepcopy(vs)
            for v in renamed_vs:
                v["name"] = grp_name + ":" + v["name"]
            mixed_vs.extend(renamed_vs)
            vs = [_ for _ in sort_values_by_confidence(vs)]
            final_results[grp_name][tgt] = vs
        mixed_vs = [_ for _ in sort_values_by_confidence(mixed_vs)]
        final_results["mixed"][tgt] = mixed_vs

    for topk in topk_list:
        if topk == -1:
            topk = int(1e10)
            print("All models per target:")
        else:
            print(f"Top {topk} models per target:")

        display_group_metrics(final_results, topk)

def display_group_metrics(final_results, topk: int):
    # print(final_results)
    topk_results = defaultdict(list)
    for model_name, per_tgt_results in final_results.items():
        for tgt, vs in per_tgt_results.items():
            scores = sort_values_by_score(vs[:topk])
            best_score = scores[0]
            topk_results[model_name].append(best_score)

    for grp_name, tgt_scores in topk_results.items():
        # Print dockq metrics
        print(grp_name)
        for met in ["dockq", "fnat", "irms"]:
            scores = [s["score"]["dockq"][met] for s in tgt_scores]
            print(grp_name, f"dockq:{met}", np.mean(scores), np.median(scores))
        # Print usalign metrics
        for met in ["tm_score1", "rmsd"]:
            scores = [s["score"]["usalign"][met] for s in tgt_scores]
            print(grp_name,f"usalign:{met}", np.mean(scores), np.median(scores))
        # Print ost metrics
        for met in ["oligo_lddt", "qs_global"]:
            scores = [s["score"]["ost"][met] for s in tgt_scores]
            print(grp_name, f"ost:{met}", np.mean(scores), np.median(scores))
        # Print iface metrics
        for met in ["ics"]:
            scores = [s["score"]["iface"]["iface_contact"][met] for s in tgt_scores]
            print(grp_name, f"ost:{met}", np.mean(scores), np.median(scores))
        for met in ["ips"]:
            scores = [s["score"]["iface"]["iface_site"][met] for s in tgt_scores]
            print(grp_name, f"ost:{met}", np.mean(scores), np.median(scores))

def iterate_score_dir(args, rank_result):
    assert args.score_path and args.score_path.exists()
    assert args.score_path.is_dir()
    score_dir = args.score_path

    for tgt_dir in tqdm(sorted(score_dir.iterdir())):
        tgt_name = tgt_dir.stem
        if tgt_name not in rank_result:
            continue
        result_dir = tgt_dir.joinpath(args.result_name)
        assert result_dir.exists()

        model_to_idx = {
            v["name"]: i for i, v in enumerate(rank_result[tgt_name])
        }
        for score_path in result_dir.glob("*.json"):
            model_name = score_path.stem
            idx = model_to_idx[model_name]
            yield idx, tgt_name, model_name, score_path

def main(args):
    if args.fix_model:
        dockq.fix_model(args.model_path, args.native_path, args.output_model_path)
        return

    if args.compare_results:
        result_paths = [Path(p.strip()) for p in args.all_result_paths.split(",")]
        mix_results(result_paths, tgt_file=args.tgt_csv)
        return

    if args.rank_iface or args.rank_openstructure:
        assert args.rank_result_path and args.rank_result_path.exists()
        with open(args.rank_result_path) as fh:
            rank_result = json.load(fh)

        all_tgts = defaultdict(dict)
        for idx, tgt_name, model_name, score_path in iterate_score_dir(args, rank_result):
            if args.rank_iface:
                score = parse_iface_result(score_path)
                rank_result[tgt_name][idx]["score"]["iface"] = score
            elif args.rank_openstructure:
                score = parse_ost_result(score_path)
                rank_result[tgt_name][idx]["score"]["ost"] = score

            all_tgts[tgt_name][model_name] = score

        if args.rank_openstructure and args.openstructure_result_path:
            with open(args.openstructure_result_path, "wt") as fh:
                json.dump(rank_result, fh, indent=2)

        if args.rank_iface and args.iface_result_path:
            with open(args.iface_result_path, "wt") as fh:
                json.dump(rank_result, fh, indent=2)

        return

    if args.model_root is not None:
        model_root = args.model_root
        result_name = args.result_name
        if args.rank_model:
            assert args.score_path
            output_root = args.score_path
        else:
            output_root = args.output_model_root

        all_scores = {}
        for model_dir in tqdm(sorted(model_root.iterdir())):
            _args = copy.deepcopy(args)
            name = model_dir.name
            _args.model_dir = model_dir.joinpath(result_name)
            if args.rank_model:
                _args.score_path = output_root.joinpath(name, result_name)
                scores = rank_models(_args)
                if scores:
                    all_scores[name] = scores
            else:
                assert output_root is not None, "Must provide output model root."
                _args.native_path = args.native_root.joinpath(f"{name}.pdb")
                _args.output_model_dir = output_root.joinpath(name, result_name)
                assess_model_dir(_args)

        if args.rank_model and args.rank_result_path:
            if all_scores:
                with open(args.rank_result_path, "wt") as fh:
                    json.dump(all_scores, fh, indent=2)
            else:
                logger.warning(f"Empty scores")
    else:
        if args.rank_model:
            rank_models(args)
        else:
            assess_model_dir(args)


if __name__ == "__main__":
    args = get_args()

    main(args)
